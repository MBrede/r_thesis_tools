[
  {
    "objectID": "prep_lit.html",
    "href": "prep_lit.html",
    "title": "Vorbereitung der Literaturrecherche",
    "section": "",
    "text": "Bitte installiert Zotero und den Zotero-Connector als Vorbereitung auf die Sitzung.",
    "crumbs": [
      "Vorwort",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vorbereitung der Literaturrecherche</span>"
    ]
  },
  {
    "objectID": "prep.html",
    "href": "prep.html",
    "title": "Vorbereitung R-Teil",
    "section": "",
    "text": "Um die Übungen um Skript ausführen zu können, braucht es einen Rechner mit aktuellen Versionen von R und RStudio. Die Installationsdateien für R für Windows findet man hier und für Mac hier, die Installationsdateien für RStudio für Windows hier und für Mac hier.\nWir brauchen mindestens RStudio 1.4 und eine entsprechende R-Version.\nZum Updaten können einfach die aktuellen installer heruntergeladen und ausgeführt werden, sonst gibt es hier auch noch eine Anleitung.\nAußerdem werden im Skript die folgenden Pakete genutzt:\n\ntidyverse\napaTables\nflextable\nezANOVA\n\nUm alle Pakete zu installieren, die im Skript genutzt werden, bitte die folgende Zeile ausführen:\n\ninstall.packages(c('tidyverse', 'apaTables', 'flextable', 'ezANOVA'), dependencies = TRUE)",
    "crumbs": [
      "Vorwort",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Vorbereitung R-Teil</span>"
    ]
  },
  {
    "objectID": "finding_literature.html",
    "href": "finding_literature.html",
    "title": "Literatur finden",
    "section": "",
    "text": "Ohne Startpunkt\nManchmal muss Literatur gesucht werden, ohne dass es ein Paper, einen Autoren oder eine Arbeit gibt, die als Ursprungspunkt für die Suche genutzt werden kann.\nZu dieser Art der Suche wurden in der Umfrage die in Abbildung 3.1 zu sehenden, groben Antwort-Kategorien angegeben.\nAbb 3.1: Grobe Kategorisierung der Antworten auf die erste Frage.\nMit den groben Kategorien sind die folgenden, ungefähren Vorgehensweisen gemeint:\nTab 3.1: Übersicht über Datenquellen. Das große Werbeargument zumindest der komerziellen Datenbanken ist ihr Redaktionsprozess. Quellen werden also auf ihre Tauglichkeit überprüft, bevor sie aufgenommen werden. Bei Crossref und OpenAlex reicht das Vorhandensein eines Identifiers. Google Scholar und Researchgate haben (zumindest nicht transparent) Auswahlprozesse für gelistete Arbeiten.\n\n\n\n\n\n\nName\nScope\nOpen Data\nZugang aus Uninetz/mit Uni-Account\n\n\n\n\nAPA PsycInfo\nDatenbank der APA - hauptsächlich psychologische Fachartikel\nNein\nJa\n\n\nPSYNDEXplus Literature and Audiovisual Media\nDatenbank des ZPID - deutschsprachig-psychologische Fachliteratur\nNein\nJa\n\n\nPubmed\nDatenbank des NCBI der US - Biologie, Medizin, für Psychologen vor allem interessant: klinische und Neuropsychologie\nJa\n\n\n\nOpenAlex\nOffene Sammlung von Forschungsarbeiten - (Fast) alle Forschungsbereiche\nJa\n\n\n\nCrossref\nOffene Sammlung von Forschungsarbeiten - (Fast) alle Forschungsbereiche\nJa (über API)\n\n\n\nScopus\nDatenbank von Elsevier - (Fast) alle Forschungsbereiche\nNein\nJa\n\n\nDimensions\nDatenbank von Digital Science - (Fast) alle Forschungsbereiche\nNein\nFrei für private Nutzung\n\n\nWeb of Science\nDatenbank von Clarivate - (Fast) alle Forschungsbereiche\nNein\nJa\n\n\nGoogle Scholar\nSuchmaschine von Google - Alle Forschungsbereiche\nNein\nNutzung kostenlos\n\n\nResearchgate\nAkademisches soziales Netzwerk - Alle Forschungsbereiche\nNein\nNutzung kostenlos\nDas genaue Item in der Umfrage war hier: “Wenn ich zu einem groben Thema recherchiere, wenn ich also zum Beispiel weiß, dass mich der Zusammenhang von Intelligenz und Schokolade interessiert, gehe ich wie folgt vor:”\nEin paar der Antworten zu den einzelnen Strategien waren über die allgemeine Beschreibung als Tipps sehr hilfreich, zum Beispiel die folgende zu Google Scholar:\nZu den Datenbanken ist folgendes Beispiel zum Einsatz von logischen Operatoren außerdem sehr hilfreich:\nJe nach Datenbank wird eine Auswahl an logischen Operatoren zugelassen, für eine Anleitung zu effektiven Suchstrategien ist die Anleitung zu Pubmed von der Johns Hopkins Universität ganz gut. Die Anleitung hat viele PubMed-spezifische Punkte, die Teile 4,5 und 6 haben aber auch gute Hinweise, die sich auf andere Datenbanken generalisieren lassen.\nAußerdem wurde in der Umfrage die etwas engere Frage gestellt, wie vorgegangen wird, wenn schon spezifische Schlagworte, die für das Thema relevant sind, bekannt sind.\nDazu waren die folgenden Hinweise ergänzend zu den bisher genannten Punkten noch sehr hilfreich:\nUnd ein Tool genannt, das zumindest ich noch nicht kannte:\nZu Connected Papers später mehr. Zuerst könnt Ihr aber versuchen die Aufgabe aus dem Fragebogen zu beantworten:",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literatur finden</span>"
    ]
  },
  {
    "objectID": "finding_literature.html#ohne-startpunkt",
    "href": "finding_literature.html#ohne-startpunkt",
    "title": "Literatur finden",
    "section": "",
    "text": "Google Scholar: Die Suche des groben Themas in der akademischen Version der beliebten Suchmaschine\nLehr-/Handbücher: Wenn schon aus dem Studium o.ä. bekannt ist, in welchem groben Themenbereich das Objekt der Suche einzuordnen ist, schadet es nicht, in entsprechenden Lehrbüchern nachzuschlagen. Die Infos sind dann aber wahrscheinlich nicht die aktuellsten.\nDatenbanken: Hiermit sind die gängigen Datenbanken zur Suche von Literatur gemeint, in Tabelle 3.1 findet Ihr eine (nicht vollständige) Übersicht über Datenbanken und andere Quellen. Bei grober Themen-Suche muss aber darauf geachtet werden, dass je nach Datenbank nicht unbedingt Synonyme mitgesucht werden.\nÜbersichtsartikel: Dieser Punkt ist nicht ohne 1 und 3 zu denken, beide Such-Möglichkeiten bieten aber die Möglichkeit, Übersichtsartikel als Suchkriterium einzustellen.\n\n\n\n\n\nBegriffe (bspw. Schokolade, Intelligenz) auf Englisch und mit möglichst vielen Synonymen in Suchmaschinen eingeben (u.a. Google Scholar, Universitätsbibliothek)\n\n\n\nich suche in verschiedenen Datenbanken nach: “Zusammenhang” AND “Intelligenz” AND “Schokolade”. Bei zu wenigen Ergebnissen mache ich meine Suche offener, z.B. “Intelligenz” AND “Schokolade”.\n\n\n\n\n\n[…] Falls ich weiß, dass ein bestimmtes Journal häufiger Studien zu diesem Thema veröffentlicht, suche ich gezielt auf den Webseiten (z.B. JSTOR, PubMed, etc.) nach dem Journal und Keywords oder nutze die Wortsuche. Sollte ich auch noch eine Jahresgrenze habe, würde ich danach filtern.\n\n\n\nIch suche nach den Autoren und Stichpunkten um das Paper zu finden. Zum Teil verwende ich connected papers.\n\n\n\nAufgabe\n\nFinde einen Übersichtsartikel, der sich mit dem Zusammenhang von Kognitiver Leistungsfähigkeit und Schokoladenkonsum auseinandersetzt.",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literatur finden</span>"
    ]
  },
  {
    "objectID": "finding_literature.html#mit-startpunkt",
    "href": "finding_literature.html#mit-startpunkt",
    "title": "Literatur finden",
    "section": "Mit Startpunkt",
    "text": "Mit Startpunkt\nDie zumindest im Rahmen der Abschlussarbeit realistischere Situation ist aber ja die, mit einer Auswahl an Ausgangspapern zu starten.\nDazu wurden in der Umfrage die Frage gestellt, wie beim Start mit einer Ausgangsarbeit vorgegangen wird. Die (wieder gruppierten) Ergebnisse sind in Abbildung 3.2 zu sehen.\n\n\n\n\n\n\n\n\nAbb 3.2: Grobe Kategorisierung der Antworten auf die dritte Frage.\n\n\n\n\n\nMit den Kategorien ist im Einzelnen das folgende gemeint:\n\nReferences durchgehen: Entweder beim Lesen des Papers Referenzen an relevanten Teilen raussuchen oder direkt die Literaturliste durchgehen und interessante Titel organisieren.\nAndere Publikationen der Autor:innen durchgucken: In Suchmaschinen oder Datenbanken nach den Autor:innen suchen und in den Publikationslisten nach anderen thematisch relevanten Publikationen suchen.\nPublikationen durchgehen, die das Paper zitiert haben: Bei z.B. dem Web of Science, Google Scholar und Pubmed gibt es die Möglichkeit, sich Paper anzeigen zu lassen, die ein Ausgangspaper zitiert haben. Diese Strategie ist insbesondere hilfreich um zum Einen aktuellere Studien zum Thema zu finden, zum Anderen um sein eigenes Paper gegen aktuelle Arbeiten abzugrenzen. Beispiele hierfür sind in Abbildung 3.3 mit roten Pfeilen eingezeichnet.\n“Ähnliche Artikel”: Bei einigen der typischen Datenbanken und Scholar gibt es eine Schaltfläche um sich ähnliche oder verwandte Artikel anzeigen zu lassen. In Abbildung 3.3 mit den blauen Pfeilen eingezeichnet.\n\n\n\n\n\n\n\n\n\nWOS\n\n\n\n\n\n\n\nGoogle Scholar\n\n\n\n\n\n\n\n\n\nPubmed\n\n\n\n\n\n\nAbb 3.3: Screenshots von den Schaltflächen zu Zitaten und ähnlichen Artikeln in Web of Science, Google Scholar und Pubmed.\n\n\n\nZu dem Item in der Umfrage war insbesondere die folgende Antwort sehr interessant:\n\n[…] Es gibt zudem Webseiten, auf denen direkt nach ähnlichen Artikeln gesucht werden kann und zusätzlich „verwandte“ Studien angezeigt werden, die sich mit einem ähnlichen Thema befasst haben.\n\nBeispiele für solche Seiten sind Digital/AI Research Assistants, die mit Hilfe von Sprachmodellen und/oder anderer Algorithmik die Literaturrecherche unterstützen. Bei der DGI gibt es einen Einführungsartikel der ganz interessant ist. Der Autorengruppe geschuldet ist er vor allem auf die Implikationen für Informationsinfrastrukturen (also Bibliotheken o.ä.) ausgelegt, die Einführung ist trotzdem ganz brauchbar.\nBeispiele die Ihr ausprobieren könnt wären:\n\nResearchrabbit (Anmeldung mit E-Mailadresse erforderlich; kostenlos)\nElicit (Anmeldung mit E-Mailadresse erforderlich; kostenlos)\nInciteful (keine Anmeldung notwendig)\n\n\nEin Punkt zur Nutzung von (KI-)Werkzeugen\nFür alle (KI-)Werkzeuge die Ihr beim Erstellen von Arbeiten nutzt gilt immer:\nIhr seid haftbar für das Ergebnis!\nEs liegt also in Eurer Verantwortung die mit KI generierten Inhalte genau zu überprüfen und anzupassen!\nKI-Assistenzen werden nie fehlerfrei arbeiten!\n\nDer in einem der Kommentare genannte Service connected papers ist auch ein digitaler Research Assistent - so weit ich das einsehen kann aber ohne Sprachmodell im Hintergrund.\nDer Service nutzt aus, dass sich Publikationen und deren Zitations-Beziehungen als mathematischer Graph darstellen lassen. Das eröffnet die Möglichkeit, zentrale Publikationen für einen Forschungsbereich über ihre Vernetzung mit den restlichen Arbeiten zu entdecken. In Abbildung 3.4 ist ein solcher Graph abgebildet.\n\n\n\n\n\n\nAbb 3.4: Screenshot von connected papers.\n\n\n\nMit Inciteful lässt sich ein ähnlicher Graph erstellt werden.\nSolltet Ihr daran Interesse haben, kann man Zitationsgraphen übrigens auch mit R generieren, dazu gibt es hier ein Beispiel wie man mit OpenCitations Daten sammelt und hier ein Beispiel wie man mit igraph einen Graphen erstellt.\n\nAufgabe\n\nRufe den Übersichtsartikel über Kognitive Leistungsfähigkeit und Schokoladenkonsum noch einmal auf. Suche auf Pubmed und Dimensions nach ähnlichen Artikeln.\nSuche auf Open Alex nach einem anderen Artikel der Erstautor:in, der dich interessiert.\nErstelle einen Graphen mit connected papers zu dem Übersichtsartikel\nErstelle einen Graphen zum selben Artikel mit Inciteful. Vergleiche die Graphen.",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Literatur finden</span>"
    ]
  },
  {
    "objectID": "collecting_literature.html",
    "href": "collecting_literature.html",
    "title": "Literatur sammeln",
    "section": "",
    "text": "Volltext-Versionen finden\nWie kann ich also vorgehen, um ein Paper zu organisieren? In der Umfrage wurden die in Abbildung 4.1 angegebenen Wege zur Organisation von Publikationen angegeben.\nAbb 4.1: Wege, an Literatur zu kommen.\nMit den Punkten sind die folgenden Vorgehensweisen gemeint:\nNeben diesen Varianten sind noch die folgenden zwei Punkte zu nennen:\nGerade der letzte Punkt löst noch ein zweites Problem und zwar die Frage, wie ich den Überblick über die gesammelten Studien behalte.",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literatur sammeln</span>"
    ]
  },
  {
    "objectID": "collecting_literature.html#volltext-versionen-finden",
    "href": "collecting_literature.html#volltext-versionen-finden",
    "title": "Literatur sammeln",
    "section": "",
    "text": "Google Scholar: Wenn ein Paper in Google Scholar gefunden ist, wird oft ein Link zu einem PDF angezeigt. Ein Beispiel ist in Abbildung 4.2 zu sehen. Scholar nutzt übrigens auch oft Researchgate um Links zu PDF-Versionen anzubieten.\nWebsite der Publisher: Der Trend geht immer mehr zu Open Access Formaten und manche Journals werden auch über die UB bereitgestellt. Ein Besuch der Publisher Website (geht am schnellsten mit “https://doi.org/”) kann also schon zum gewünschten Ergebnis führen. OA-Paper werden aber auch schon von Google gelistet\nResearchgate: Viele Autor:innen laden ihre Arbeiten auf Researchgate als Volltext hoch, auch wenn das Journal eigentlich nicht OA ist.\nDirekt Autor anschreiben: Zuletzt funktioniert es oft, dem Kontaktautoren eine Email zu schreiben. Mit der Veröffentlichung erhalten Autor:innen bei vielen Journals eine PDF, die sie oft gern weitergeben. Über Researchgate lässt sich eine derartige Anfrage übrigens auch stellen.\n\n\n\n\n\n\n\nAbb 4.2: PDF auf Google Scholar\n\n\n\n\n\nUnpaywall: Ein Service, der offene Zugänge zu Publikationen sammelt und hier Browser-Extensions zur Verfügung stellt, die anzeigen, wenn ein Paper kostenfrei zugänglich ist.\nLiteraturverwaltungsprogramme: Viele Literaturverwaltungsprogramme nutzen Verzeichnisse von offen zugänglichen Publikationen, die einfach mit einer DOI zugägnlich gemacht sind. Bei Hinzufügen einer Publikation via DOI oder Browser-Extension wird bei Zotero zum Beispiel direkt versucht, eine PDF-Version der Arbeit herunterzuladen.",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literatur sammeln</span>"
    ]
  },
  {
    "objectID": "collecting_literature.html#den-überblick-behalten",
    "href": "collecting_literature.html#den-überblick-behalten",
    "title": "Literatur sammeln",
    "section": "Den Überblick behalten",
    "text": "Den Überblick behalten\nNachdem wir nun interessante Paper gefunden und uns vielleicht die Volltexte organisiert haben entsteht irgendwann die Frage: Wie behalte ich den Überblick?\nDie Lösung dieses Problem sind Literaturverwaltungsprogramme, zu deren Nutzung in der Umfrage ein Item präsentiert wurde - die Ergebnisse sind in Abbildung 4.3 zu sehen.\n\n\n\n\n\n\n\n\nAbb 4.3: In der Umfrage als genutzt angegebene Referenzmanager.\n\n\n\n\n\nViele von Euch haben sich also noch nicht zur Nutzung durchgerungen, deswegen hier die Vorteile:\n\nIhr behaltet den Überblick und könnt Referenzen organisieren\nIhr könnt das Tool beim Suchen mit Browser-Extensions die Referenz abspeichern lassen\nIhr könnt Euch im Verwaltungs-Tool Tags und Notizen zu machen\n\nUnd der wichtigste:\n\nDas Tool formatiert Literaturhinweise für Euch (!)\n\nAn Tools aus dieser Familie gibt es eine ganze Reihe, in Tabelle 4.1 sind ein paar vorgestellt. Hier nutzen wird Zotero, da es auf allen Rechnern funktioniert, geteilte Bibliotheken erlaubt und Open Source ist. Eine Anleitung zur Nutzung mit den gebräuchlisten Word-Prozessoren findet Ihr hier.\n\n\n\n\nTab 4.1: Übersicht über Referenzmanager.\n\n\n\n\n\n\nName\nAnbieter\nLäuft auf\nLizenzmodell\n\n\n\n\nZotero\nOpen Source Community\nallem\nOpen Source\n\n\nMendeley\nElsevier\nWindows, macOS, Linux, Android, iOS\nFreeware\n\n\nCitavi\nSwiss Academic Software\nWindows\nBezahlmodell - von CAU gestellt\n\n\nEndnote\nClarivate\nWindows, macOS\nBezahlmodell\n\n\n\n\n\n\n\n\n\nAufgabe\n\nInstalliert Zotero und den auf der selben Seite angebotenen Connector, sofern Ihr das noch nicht habt\nFügt dieses Paper zu Eurer Sammlung hinzu\nErstellt ein Dokument im Textverarbeitungsprogramm Eures Vertrauens, in dem Ihr das Paper zitiert. Lasst Euch eine Reference-Liste mit APA-Formatierung erstellen.",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Literatur sammeln</span>"
    ]
  },
  {
    "objectID": "reading_literature.html",
    "href": "reading_literature.html",
    "title": "Literatur lesen",
    "section": "",
    "text": "Notizen machen\nEine zentrales Problem, das zumindest mir in meiner akademischen Arbeit untergekommen ist, ist dass ich ein Paper lese und wenn ich ans Schreiben gehe im besten Fall nur noch grob weiß, wo was stand.\nDeswegen versuche ich inzwischen, mir beim Lesen eines Papers Notizen zu machen, die ich später wiederfinden und mit dem Paper verknüpfen kann.\nIn der Umfrage ergab sich, dass 83% der Antwortenden auch Notizen machen, das Ausmaß und der Inhalt unterscheiden sich aber sehr deutlich.\nDabei waren sich, wie in Abbildung 6.1 zu sehen ist, die meisten über das Tool für diesen Zweck aber einig.\nAbb 6.1: Methoden zum Notizen machen.\nDazu haben außerdem 75% der Abstimmenden angegeben, dass sie sich Textstellen farblich markieren.\nAlle diese Methoden sind gut und die letzten Ergebnisse die ich gesehen habe, deuten darauf hin, dass für das Lernen Notizen erstellen und Textstellen Markieren gleich effizient sind (z.B.: Leonard et al., 2021)\nFür mich hat in letzter Zeit aber ganz gut funktioniert, Teile des Zettelkasten-Systems zu übernehmen.\nBeim Zettelkasten-System werden (je nach Quelle ist die Beschreibung etwas unterschiedlich) “atomare” Notizen, also maximal ein-zwei Sätze lange Zusammenfassungen erstellt. Dabei ist 1. wichtig, dass eine Referenz auf den Originaltext enthalten ist, 2. dass die Zusammenfassung in eigenen Worten geschieht.\nIm eigentlichen Zettelkasten-Prinzip gibt es dann noch Vereinfachungsschritte und Filter-Prozesse für die Notizen, die dazu beitragen sollen den eigenen Gedanken für die eigene Arbeit zu destillieren, diesen Schritt lasse ich zumindest aber oft einfach aus.\nDer Vorteil beim Schreiben ist dann aber, dass ich meine Notizen direkt durchsuchen kann und mit dem Link zur Publikation die Referenz für das Argument zur Hand habe.\nFür dieses System bietet es sich an entweder direkt Notizen in Zotero anzulegen, die dann auch in Zotero durchsuchbar sind. Alternativ bieten sich Notiz-Apps wie Joplin und Obsidian an, die auch eine fetzige Graph-Repräsentation der Notizen ermöglichen.",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Literatur lesen</span>"
    ]
  },
  {
    "objectID": "reading_literature.html#literatur",
    "href": "reading_literature.html#literatur",
    "title": "Literatur lesen",
    "section": "Literatur",
    "text": "Literatur\n\n\n\n\nLeonard, S., Stroud, M. J., & Shaw, R. J. (2021). Highlighting and taking notes are equally ineffective when Reading paper or eText. Education and Information Technologies, 26(4), 3811–3823. https://doi.org/10.1007/s10639-021-10448-9",
    "crumbs": [
      "Literaturrecherche",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Literatur lesen</span>"
    ]
  },
  {
    "objectID": "intro_r.html#footnotes",
    "href": "intro_r.html#footnotes",
    "title": "Thesis mit R",
    "section": "",
    "text": "weil SPSS doof ist.↩︎",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Thesis mit R</span>"
    ]
  },
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Import und Aufbereitung von Psychopy-Daten",
    "section": "",
    "text": "Batch-Import\nPsychopy und Pavlovia schreiben Ihre Daten im long-Format raus, die dem kommenden Beispiel ähneln:\nread_csv('data/1_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv') %&gt;% \n  head()\n\nNew names:\nRows: 47 Columns: 20\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(9): Bild, weiter_Willkommen.keys, Ergebnisse, Sicherheit, key_resp.key... dbl\n(10): Ergebnisse_Loop.thisRepN, Ergebnisse_Loop.thisTrialN, Ergebnisse_L... lgl\n(1): ...20\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...20`\n\n\n# A tibble: 6 × 20\n  Bild       Ergebnisse_Loop.this…¹ Ergebnisse_Loop.this…² Ergebnisse_Loop.thisN\n  &lt;chr&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n1 &lt;NA&gt;                           NA                     NA                    NA\n2 Tabellen/…                      0                      0                     0\n3 Tabellen/…                      0                      1                     1\n4 Tabellen/…                      0                      2                     2\n5 Tabellen/…                      0                      3                     3\n6 Tabellen/…                      0                      4                     4\n# ℹ abbreviated names: ¹​Ergebnisse_Loop.thisRepN, ²​Ergebnisse_Loop.thisTrialN\n# ℹ 16 more variables: Ergebnisse_Loop.thisIndex &lt;dbl&gt;,\n#   weiter_Willkommen.keys &lt;chr&gt;, weiter_Willkommen.rt &lt;dbl&gt;,\n#   Entscheidung.response &lt;dbl&gt;, Ergebnisse &lt;chr&gt;,\n#   Sicherheit_Entscheidung.response &lt;dbl&gt;, Sicherheit &lt;chr&gt;,\n#   key_resp.keys &lt;chr&gt;, key_resp.rt &lt;dbl&gt;, participant &lt;dbl&gt;, session &lt;chr&gt;,\n#   date &lt;chr&gt;, expName &lt;chr&gt;, psychopyVersion &lt;chr&gt;, frameRate &lt;dbl&gt;, …\nDie Daten oben kommen aus einer FOV-Studie aus dem WS21 in der untersucht wurde, wie sicher sich Proband:innen bei der Entscheidung für einen t-Test oder einen anderen Test basierend auf Levene- und KS-Test-p-Werten sind und wie sehr sie sich in ihrer Entscheidung sicher sind..\nFür jeden Probanden wird dabei eine Datei erstellt, so dass der data-folder gerne mal wie in Abbildung 7.1 aussehen kann.\nUm Auswertungen vorzubereiten müssen diese einzelnen Dateien erst mal zusammengefügt werden.\nDazu brauchen wir erst mal einen Vektor mit allen csv-files im Ordner:\n# bei mir liegt der data-Ordner im Verzeichnis über dem in dem sich das Skript befindet\nlist.files(path = 'data/',\n           pattern = 'csv') %&gt;% \n  str()\n\n chr [1:51] \"1_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv\" ...\nDa bei mir ein relativer Pfad nötig ist würden die files so nicht gefunden werden, deswegen müssen wir noch das full.names-Argument auf TRUE setzen.\nlist.files(path = 'data/',\n           pattern = 'csv',\n           full.names = T) %&gt;% \n  str()\n\n chr [1:51] \"data//1_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv\" ...\nDie so erstellten Pfade können wir dann im batch öffnen und einlesen. Dazu kann der wrapper map_dfr um map und bind_rows aus dem purrr-Paket helfen:\nall_vpn &lt;- list.files(path = 'data/',\n           pattern = 'csv',\n           full.names = T) %&gt;% \n  map_dfr(~read_csv(.))\n\nError in `dplyr::bind_rows()`:\n! Can't combine `..26$Ergebnisse_Loop.thisRepN` &lt;double&gt; and `..27$Ergebnisse_Loop.thisRepN` &lt;character&gt;.\nDer Fehler kommt daher, dass einzelne Dokumente zu kurz sind und die Spaltentypen deswegen nicht erkannt werden. Hier gibt es drei Möglichkeiten mit dem Problem umzugehen:\nBei Psychopy kann der Fehler oben nur auftreten wenn\nIm ersten Fall habt Ihr ganz andere Probleme und solltet im Detail darüber nachdenken wie ihr das löst - oder den unschönen Weg (“The ugly”) wählen.\nIm zweiten Fall könnt Ihr alle zu kleinen files filtern (“The good”) oder manuell (“The bad”) die Spalten definieren.\nMit dem so umgewandelten Datensatz können wir dann wie gewohnt weiterarbeiten.\nZum Beispiel können wir uns die durchschnittliche Sicherheit pro Proband:in und Entscheidung ausgeben lassen:\nresponse_summary &lt;- df %&gt;% \n  group_by(participant, Ergebnisse) %&gt;% \n  summarise(Sicherheit = mean(Sicherheit_Entscheidung.response))\n\n`summarise()` has grouped output by 'participant'. You can override using the\n`.groups` argument.\n\nresponse_summary %&gt;% \n  head()\n\n# A tibble: 6 × 3\n# Groups:   participant [3]\n  participant Ergebnisse   Sicherheit\n        &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1           1 anderer Test       2.65\n2           1 t-Test             2.57\n3           2 anderer Test       2.33\n4           2 t-Test             2.52\n5           3 anderer Test       2.18\n6           3 t-Test             2.54",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import und Aufbereitung von Psychopy-Daten</span>"
    ]
  },
  {
    "objectID": "wrangling.html#batch-import",
    "href": "wrangling.html#batch-import",
    "title": "Import und Aufbereitung von Psychopy-Daten",
    "section": "",
    "text": "Abb 7.1: Ansicht des Daten-Ordners\n\n\n\n\n\n\n\n\n\n\n\n\n\netwas am Skript geändert wurde wodurch ein Datentyp in einer Datei nicht mehr stimmt.\nein:e Proband:in vor dem ersten Trial eines Blocks abgebrochen hat.\n\n\n\n\nThe goodThe badThe ugly\n\n\n\nfiles &lt;- tibble(\n  path = list.files(\n    path = 'data/',\n    pattern = 'csv',\n    full.names = T\n  ),\n  size = file.size(path)\n)\n\nfiles %&gt;% \n  head()\n\n# A tibble: 6 × 2\n  path                                                      size\n  &lt;chr&gt;                                                    &lt;dbl&gt;\n1 data//1_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv   6597\n2 data//10_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv  6617\n3 data//11_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv  6609\n4 data//12_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv  6598\n5 data//13_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv  6599\n6 data//14_Experiment_Vorüberlegungen_2022_Feb_14_1902.csv  6594\n\nall_vpn &lt;- files %&gt;% \n  filter(size &gt; mean(size)) %&gt;% \n  pull(path) %&gt;% \n  map_dfr(~read_csv(.))\n\n\n\nDie zweite Möglichkeit ist es, in read_csv die erwarteten Spalten-Typen anzugeben.\nIn meinem Fall sieht das so aus:\n\nall_vpn &lt;- list.files(path = 'data/',\n           pattern = 'csv',\n           full.names = T) %&gt;% \n  map_dfr(~read_csv(.,col_types = 'cnnnncnncnccnnccccnl'))\n\n\n\nDer uneleganteste Weg ist es erstmal alle Spalten als character zu importieren und dann die nötigen Spalten umzuwandeln:\n\nall_vpn &lt;- list.files(path = 'data/',\n           pattern = 'csv',\n           full.names = T) %&gt;% \n  map_dfr(~read_csv(.,col_types = cols(.default = 'c')))\n\nall_vpn %&gt;% \n  glimpse()\n\nRows: 20,629\nColumns: 23\n$ Bild                             &lt;chr&gt; NA, \"Tabellen/45.png\", \"Tabellen/11.p…\n$ Ergebnisse_Loop.thisRepN         &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"…\n$ Ergebnisse_Loop.thisTrialN       &lt;chr&gt; NA, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"…\n$ Ergebnisse_Loop.thisN            &lt;chr&gt; NA, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"…\n$ Ergebnisse_Loop.thisIndex        &lt;chr&gt; NA, \"44\", \"10\", \"32\", \"40\", \"16\", \"27…\n$ weiter_Willkommen.keys           &lt;chr&gt; \"space\", NA, NA, NA, NA, NA, NA, NA, …\n$ weiter_Willkommen.rt             &lt;chr&gt; \"5.486211061477661\", NA, NA, NA, NA, …\n$ Entscheidung.response            &lt;chr&gt; NA, \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ Ergebnisse                       &lt;chr&gt; NA, \"anderer Test\", \"anderer Test\", \"…\n$ Sicherheit_Entscheidung.response &lt;chr&gt; NA, \"1\", \"3\", \"4\", \"1\", \"3\", \"3\", \"1\"…\n$ Sicherheit                       &lt;chr&gt; NA, \"sehr sicher\", \"unsicher\", \"gar n…\n$ key_resp.keys                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ key_resp.rt                      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ participant                      &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1…\n$ session                          &lt;chr&gt; \"001\", \"001\", \"001\", \"001\", \"001\", \"0…\n$ date                             &lt;chr&gt; \"2022_Feb_14_1853\", \"2022_Feb_14_1853…\n$ expName                          &lt;chr&gt; \"Experiment_Vorüberlegungen\", \"Experi…\n$ psychopyVersion                  &lt;chr&gt; \"2021.2.3\", \"2021.2.3\", \"2021.2.3\", \"…\n$ frameRate                        &lt;chr&gt; \"59.783176973314745\", \"59.78317697331…\n$ ...20                            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ RT                               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Accuracy                         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ group                            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nAus diesem Datensatz brauchen wählen wir dann die Spalten aus, die wir brauchen und wandeln die Reaktionszeiten um (und filtern leere Zeilen).\n\ndf &lt;- all_vpn %&gt;% \n  select(participant, Entscheidung.response, Ergebnisse, Sicherheit_Entscheidung.response, Sicherheit)\nglimpse(df)\n\nRows: 20,629\nColumns: 5\n$ participant                      &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1…\n$ Entscheidung.response            &lt;chr&gt; NA, \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ Ergebnisse                       &lt;chr&gt; NA, \"anderer Test\", \"anderer Test\", \"…\n$ Sicherheit_Entscheidung.response &lt;chr&gt; NA, \"1\", \"3\", \"4\", \"1\", \"3\", \"3\", \"1\"…\n$ Sicherheit                       &lt;chr&gt; NA, \"sehr sicher\", \"unsicher\", \"gar n…\n\ndf &lt;- df %&gt;% \n  mutate(across(matches('response'), ~as.numeric(.)),\n         participant = as.numeric(participant))\n\ndf &lt;- df %&gt;% \n  filter(!is.na(Ergebnisse))\n\ndf %&gt;% \n  glimpse()\n\nRows: 2,205\nColumns: 5\n$ participant                      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Entscheidung.response            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2…\n$ Ergebnisse                       &lt;chr&gt; \"anderer Test\", \"anderer Test\", \"ande…\n$ Sicherheit_Entscheidung.response &lt;dbl&gt; 1, 3, 4, 1, 3, 3, 1, 3, 1, 2, 2, 3, 1…\n$ Sicherheit                       &lt;chr&gt; \"sehr sicher\", \"unsicher\", \"gar nicht…",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import und Aufbereitung von Psychopy-Daten</span>"
    ]
  },
  {
    "objectID": "wrangling.html#zusammenfügen",
    "href": "wrangling.html#zusammenfügen",
    "title": "Import und Aufbereitung von Psychopy-Daten",
    "section": "Zusammenfügen",
    "text": "Zusammenfügen\nIn den meisten Fällen werdet Ihr neben Psychopy-Daten noch wo anders Fragebogen dargeboten haben, die Ihr mit den Daten zusammenfügen müsst.\nIn unserem Beispiel existiert eine .xlsx-Datei, die die Limesurvey-Ergebnisse beinhaltet.\nUm die zusammengefassten Ergebnisse an diese anzufügen müssen wir sie zuerst importieren:\n\nlimesurvey_results &lt;- openxlsx::read.xlsx('data/LimeSurvey Daten.xlsx')\n\nlimesurvey_results %&gt;% \n  glimpse()\n\nRows: 50\nColumns: 4\n$ Bitte.geben.Sie.Ihr.Geschlecht.an. &lt;chr&gt; \"weiblich\", \"weiblich\", \"männlich\",…\n$ Bitte.geben.Sie.Ihr.Alter.an.      &lt;dbl&gt; 26, 18, 21, 21, 32, 32, 29, 29, 24,…\n$ `Was.machen.Sie.beruflich?`        &lt;chr&gt; \"Studium\", \"Studium\", \"Psychotherap…\n$ `VP-Nr`                            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …\n\n\nDie Zusammenfassungen sind noch im long-Format, können also nicht so einfach mit den LS-Daten im wide-Format zusammengeführt werden. Also erst einmal pivotieren:\n\nresponse_summary &lt;- response_summary %&gt;% \n  pivot_wider(names_from = 'Ergebnisse',\n              values_from = 'Sicherheit')\n\nresponse_summary %&gt;% \n  head()\n\n# A tibble: 6 × 3\n# Groups:   participant [6]\n  participant `anderer Test` `t-Test`\n        &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1           1           2.65     2.57\n2           2           2.33     2.52\n3           3           2.18     2.54\n4           4           2.29     2.57\n5           5           2.24     2.54\n6           6           1.94     2.48\n\n\nDie beiden Ergebnisse können wir jetzt kombinieren:\n\nvp_level_data &lt;- response_summary %&gt;% \n  left_join(limesurvey_results,\n            by = c('participant' = 'VP-Nr'))\n\nvp_level_data %&gt;% \n  glimpse()\n\nRows: 49\nColumns: 6\nGroups: participant [49]\n$ participant                        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …\n$ `anderer Test`                     &lt;dbl&gt; 2.647059, 2.333333, 2.176471, 2.294…\n$ `t-Test`                           &lt;dbl&gt; 2.571429, 2.518519, 2.535714, 2.571…\n$ Bitte.geben.Sie.Ihr.Geschlecht.an. &lt;chr&gt; \"weiblich\", \"weiblich\", \"männlich\",…\n$ Bitte.geben.Sie.Ihr.Alter.an.      &lt;dbl&gt; 26, 18, 21, 21, 32, 32, 29, 29, 24,…\n$ `Was.machen.Sie.beruflich?`        &lt;chr&gt; \"Studium\", \"Studium\", \"Psychotherap…\n\n\nAuf den Daten können wir dann arbeiten:\n\nvp_level_data %&gt;% \n  select(where(is.numeric)) %&gt;% \n  cor()\n\n                              participant anderer Test      t-Test\nparticipant                    1.00000000  -0.29763927 -0.01798315\nanderer Test                  -0.29763927   1.00000000 -0.35251805\nt-Test                        -0.01798315  -0.35251805  1.00000000\nBitte.geben.Sie.Ihr.Alter.an. -0.09077279  -0.07051619  0.08474383\n                              Bitte.geben.Sie.Ihr.Alter.an.\nparticipant                                     -0.09077279\nanderer Test                                    -0.07051619\nt-Test                                           0.08474383\nBitte.geben.Sie.Ihr.Alter.an.                    1.00000000",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import und Aufbereitung von Psychopy-Daten</span>"
    ]
  },
  {
    "objectID": "wrangling.html#aufgabe",
    "href": "wrangling.html#aufgabe",
    "title": "Import und Aufbereitung von Psychopy-Daten",
    "section": "Aufgabe",
    "text": "Aufgabe\n\nIm Repo zu diesem Skript ist eine zip-Datei mit simulierten Daten eines fiktiven Reaktionszeit-Experiments zu finden. Lade die Zip herunter und entpacke sie.\nImportiere die csv-Dateien in einen Datensatz\nBerechne Accuracy, mittlere Reaktionszeit und Standardabweichung der Reaktionszeit pro Proband:in\nFüge die Ergebnisse mit der LimeSurvey-xlsx zusammen und erstelle eine Zusammenfassung der Reaktionszeiten und Accuracy pro angegebenem Geschlecht",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import und Aufbereitung von Psychopy-Daten</span>"
    ]
  },
  {
    "objectID": "graphics.html",
    "href": "graphics.html",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "",
    "text": "Grammar of graphics\nHadley Wickhams Paket ggplot2 versucht, die Erstellung von Grafiken in einer einheitlichen Grammatik, der “grammar of graphics”, auszudrücken. Das Ziel hier ist es, nicht mehr in “Scatterplot” und “Boxplot” als einzelne Kategorien zu denken und diese einzeln erstellen lernen zu müssen, sondern alle Abbildungen mit derselben Logik erstellen zu können.\nIn Seinem Paper (Wickham, 2010) werden die folgenden Komponenten als grundlegende Bausteine einer Grafik eingeführt:",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#grammar-of-graphics",
    "href": "graphics.html#grammar-of-graphics",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "",
    "text": "a default dataset and set of mappings from variables to aesthetics,\none or more layers, with each layer having one geometric object, one statistical trans- formation, one position adjustment, and optionally, one dataset and set of aesthetic mappings,\none scale for each aesthetic mapping used,\na coordinate system,\nthe facet specification. (Wickham, 2010, p. 8)\n\n\n\nKomponenten eines Plots\nWir müssen für einen Plot also überlegen:\n\nwelche Daten wir auf welche Aesthetics mappen\nwelche geometrischen Objekte wir in welcher Reihenfolge auf die Grafik layer wollen und ob diese optionale andere Daten benötigen\nwelche Skala wir für die Mappings nutzen wollen\nwelches Koordinatensystem wir nutzen wollen\nin welchen Facetten wir die Daten darstellen wollen",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#komponenten-in-ggplot2",
    "href": "graphics.html#komponenten-in-ggplot2",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "Komponenten in ggplot2",
    "text": "Komponenten in ggplot2\n\nBeispieldaten\n\n\n\n\n\nPinguine im Eis\n\n\n\nIm palmerpenguins-Paket werden Pinguin-Beobachtungen der Palmer-Station in der Antarktis zur Verfügung gestellt:\n\npalmerpenguins::penguins %&gt;% \n  head()\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n1. Daten und Aesthetics - ggplot() + aes()\nWir wollen den Zusammenhang zwischen Körpergewicht und Schnabellänge über die Spezies betrachten. Dafür legen wir die “Leinwand” des Plots mit den zentralen mappings an:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species))\n\n\n\n\n\n\n\n\n\n\n2. Geometrische Objekte - geom_*\nDiesem Plot fügen wir Punkte als geometrische Objekte hinzu, die uns zu einem Scatterplot führen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nWir als weiteres geometrisches Objekt könnten wir uns zum Beispiel wünschen, die Mittelwerte pro Gruppe mit den Abweichungen auf x- und y-Achse darzustellen. Dazu müssen wir zuerst diesen neuen Datensatz berechnen:\n\npenguin_means &lt;- palmerpenguins::penguins %&gt;% \n  group_by(species) %&gt;% \n  summarise(across(c(bill_length_mm, body_mass_g), ~mean(., na.rm=T)))\n\n…und auf den Plot in einem neuen Layer hinzufügen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point() +\n  geom_point(data=penguin_means)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nFür den Layer können wir auch speifische geometrische Eigenschaften einfügen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point() +\n  geom_point(data=penguin_means, shape = 3)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nOder direkt ein neues Mapping einführen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original')) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'))\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nUnd auch beide Varianten kombinieren:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n3. Skalen - scale_*\nDie Symbole und Farben haben genau wie x- und y- Koordinaten als ästhetische Mappings eigene Skalen. Wenn uns also die Farben nicht passen, können wir einfach eine andere Skala setzen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nGenauso können wir einfach die Skala der Symbole an unsere Wünsche anpassen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(original = 20, mean = 3))\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n4. Koordinatensystem coord_*\nDas Koordinatensystem passt von der Auflösung erstmal, aber wir wollen eine direkte Zuordnung von 10 mm Schnabellänge zu 1000 g Körpermasse. Dazu fügen wir eine coord_*-Spezifikation an:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(original = 20, mean = 3))+\n  coord_fixed(ratio = 10/1000)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n5. Facetten - facet_*\nAls letzte Komponente überlegen wir uns, dass die verschiedenen Beobachtungspunkte als Einteilung interessant sein könnten und wir diese getrennt betrachten wollen. Um diese Facettierung umzusetzen ergänzen wir zuerst den Mittelwerts-Datensatz um den Beobachtungsort:\n\npenguin_means &lt;- palmerpenguins::penguins %&gt;% \n  group_by(species, island) %&gt;% \n  summarise(across(c(bill_length_mm, body_mass_g), ~mean(., na.rm=T)))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\nUm dem Graphen anschließend die Facettierung anzufügen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(original = 20, mean = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#apa-styling",
    "href": "graphics.html#apa-styling",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "APA-Styling",
    "text": "APA-Styling\nAus Julias Folien1 kommt folgende Checkliste für APA-Grafiken:\n\n\n\nScreenshot der APA-Checkliste\n\n\nWir müssen also noch\n\ndie Elemente klar labeln\nsicherstellen dass der Font passt\ndie Legende unter dem Bild anordnen\ndie Beschriftung der Legende überprüfen\n\nDaneben habe ich noch ein persönliches Problem mit dem grauen Hintergrund, damit fangen wir an:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(original = 20, mean = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island) +\n  theme_light()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn diesem Zusammenhang können wir auch gleich Base-Font und Schriftgröße setzen. theme_light setzt die kleinste Schriftgröße auf .8 * die base_size, wenn wir minimal 8pt große Schrift haben wollen.\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(original = 20, mean = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island) +\n  theme_light(base_family = 'Helvetica',\n              base_size = 10)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nAls nächstes die Anpassung der Achsen- und Legenden-Labels:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'mean'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(original = 20, mean = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island) +\n  theme_light(base_family = 'Helvetica',\n              base_size = 10) +\n  labs(x = 'Schnabellänge (mm)',\n       y = 'Körpergewicht (g)',\n       color = 'Pinguin-Spezies',\n       shape = 'Aggregations-Niveau')\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nDie Namen der Formen sind noch nicht title-cased, das können wir am einfachsten in der Definition ändern:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'Original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'Mittelwert'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(Original = 20, Mittelwert = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island) +\n  theme_light(base_family = 'Helvetica',\n              base_size = 10) +\n  labs(x = 'Schnabellänge (mm)',\n       y = 'Körpergewicht (g)',\n       color = 'Pinguin-Spezies',\n       shape = 'Aggregations-Niveau')\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nDie Legende muss dann noch an die Unterseite des Graphen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'Original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'Mittelwert'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(Original = 20, Mittelwert = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island) +\n  theme_light(base_family = 'Helvetica',\n              base_size = 10) +\n  labs(x = 'Schnabellänge (mm)',\n       y = 'Körpergewicht (g)',\n       color = 'Pinguin-Spezies',\n       shape = 'Aggregations-Niveau') +\n  theme(legend.position = 'bottom')\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nUnd damit ist die Formatierung fertig.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#convenient-standards",
    "href": "graphics.html#convenient-standards",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "Convenient Standards",
    "text": "Convenient Standards\nDie beiden theme-Funktionen müssten wir so an jede Grafik anfügen. Solche Wiederholungen sind schlechter Stil und stören beim Lesen des Skripts, deswegen bietet ggplot2 convenience-Funktionen um allgemeine Einstellungen zu setzen. Mit dem folgenden Snippet am Anfang des Skripts werden die Standards für alle Grafiken genutzt:\n\nmy_theme &lt;-  theme_light(base_family = 'Helvetica',\n              base_size = 10) +\n  theme(legend.position = 'bottom')\n\ntheme_set(my_theme)\n\nSo kann ich jetzt einfach Beispielsweise ein eingefärbtes Histogramm für die Flossen-Länge mit den gesetzten Standards erstellen:\n\npalmerpenguins::penguins %&gt;% \n  ggplot(aes(x = flipper_length_mm,\n             fill = sex)) +\n  geom_histogram(binwidth = 5)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#export",
    "href": "graphics.html#export",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "Export",
    "text": "Export\nZum Abschluss können wir die Grafiken exportieren.\nDie Textgröße ist in pt gesetzt, deswegen sollten wir nach dem Export die Größe im besten Fall nicht mehr viel ändern.\nEine Din A4-Seite ist 8.2 x 11.6 Zoll groß. Wenn wir eine Grafik auf 80% der Seitenbreite haben wollen, brauchen wir also eine 6.56 Zoll breite Grafik.\nZum Speichern setzen wir unsere Grafik und die Maße in ggsave ein:\n\np &lt;- palmerpenguins::penguins %&gt;% \n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g,\n             color = species)) + \n  geom_point(aes(shape = 'Original'),\n             alpha = .5) +\n  geom_point(data=penguin_means,\n             aes(shape = 'Mittelwert'),\n             size = 3) +\n  scale_color_viridis_d(end = 0.7) +\n  scale_shape_manual(values = c(Original = 20, Mittelwert = 3))+\n  coord_fixed(ratio = 10/1000) +\n  facet_wrap(~island) +\n  labs(x = 'Schnabellänge (mm)',\n       y = 'Körpergewicht (g)',\n       color = 'Pinguin-Spezies',\n       shape = 'Aggregations-Niveau')\n\nggsave(plot = p,\n       filename = 'imgs/penguin_scatter.png',\n       width = 6.56,units = 'in')\n\nSaving 6.56 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\nDer Export sieht so aus:\n Erstens können wir schonmal feststellen dass die Grafik ruhig schmaler werden kann. Die Export-Funktion hat uns eine Höhe von 6.74 Inches mitgeteilt, das können wir ruhig auf 4 inches reduzieren.\nZweitens ist die Legende ein bisschen kaputt gegangen.\nDazu gibt es drei Möglichkeiten zur Anpassung:\n\nLegende mit ZeilenumbrüchenLegende mit kleinerem TextLegende im Graphen\n\n\n\np_linebreaks &lt;- p +\n  guides(color = guide_legend(nrow = 3),\n         shape = guide_legend(nrow = 2))\n\nggsave(plot = p_linebreaks,\n       filename = 'imgs/penguin_scatter_linebreaks.png',\n       width = 6.56, height = 4, units = 'in')\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nGrafik mit Legende mit Linebreaks\n\n\n\n\n\np_smaller_text &lt;- p +\n  theme(legend.title = element_text(size = 8,\n                                    colour = 'darkgrey'))\n\nggsave(plot = p_smaller_text,\n       filename = 'imgs/penguin_scatter_smaller_text.png',\n       width = 6.56, height = 4,units = 'in')\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nGrafik mit kleineren Legenden-Überschriften\n\n\n\n\n\np_legend_in_plot &lt;- p + \n  theme(\n    # Legende an oberer rechter Ecke orientieren\n    legend.justification=c(1,1),\n    # Legende an obere rechte Ecke schieben\n    legend.position=c(1,0.98), \n    # Legenden-Hintergrund fast transparent machen\n    legend.background = element_rect(fill = rgb(1,1,1,.5)),\n    # Hintergrund der Legenden-Felder fast transparent machen\n    legend.key = element_rect(fill = rgb(1,1,1,.5)))\n\nggsave(plot = p_legend_in_plot,\n       filename = 'imgs/penguin_scatter_legend_in_plot.png',\n       width = 6.56, height = 4,units = 'in')\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nGrafik mit Legende in plot-Region",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#hilfreiche-links",
    "href": "graphics.html#hilfreiche-links",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "Hilfreiche Links",
    "text": "Hilfreiche Links\n\nfür einen Überblick über alle möglichen Kompenenten empfiehlt sich das von posit herausgegebene cheatsheet\ndas Kapitel zu Datenvisualisierungen in Grolemund & Wickham (2016) ist sehr gut und geht weiter ins Detail als hier möglich ist\nIm Paket ggpubr wird ggplot2 genutzt um eine Reihe von “publication ready plots” zu erstelllen\nUnter diesem Link ist eine shiny-App zur interaktiven Erstellung von ggplot-Graphen zu finden\nUnter diesem Link findet sich eine Sammlung von Farben, Formen, usw., die mit ggplot nutzbar sind.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#aufgabe",
    "href": "graphics.html#aufgabe",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "Aufgabe",
    "text": "Aufgabe\n\nLese den im Repo zu diesem Skript zur Verfügung gestellten Datensatz example.csv ein. Dazu kann einfach der folgende Chunk genutzt werden:\n\n\nread_csv('https://raw.githubusercontent.com/MBrede/r_thesis_tools/main/data/example.csv')\n\n\nStelle die Reaktionszeiten und Accuracies in einem Scatterplot dar.\nFärbe den Graphen nach Gruppen ein\nFüge Mittelwerte und Standardabweichungen pro Gruppe hinzu. Füge die Standardabweichungen dabei mit geom_linerange in zwei layern hinzu (einem für die x- und einem für die y-Richtung)\nPasse die Grafik so an, dass sie APA-konform ist\nMache die Grafik so unästhetisch, wie es die APA-Richtlinien zulassen. Hier sind Fonts, Farben und Formen zu finden.\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2016). R for Data Science. https://r4ds.had.co.nz/\n\n\nWickham, H. (2010). A layered grammar of graphics. Journal of Computational and Graphical Statistics, 19(1), 3–28.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "graphics.html#footnotes",
    "href": "graphics.html#footnotes",
    "title": "Die Grammar of graphics und ggplot2",
    "section": "",
    "text": "Danke Julia!↩︎",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Die *Grammar of graphics* und `ggplot2`</span>"
    ]
  },
  {
    "objectID": "tables.html",
    "href": "tables.html",
    "title": "Erstellung von APA-konformen Tabellen",
    "section": "",
    "text": "Export mit apaTables\nWie am Anfang gesagt wollen wir möglichst vermeiden, Tabellen und Daten a) händisch zu übertragen und b) zu formatieren.\nIm besten Fall exportieren wir also alles was an Zahlen und Tabellen für den Text anfällt direkt in files, die wir einfach einbinden können.\nFür die ANOVAs, Regressionen, t-Tests und Korrelationsanalysen gibt es im apaTables-Paket fertige Wrapper, die einen direkten Export der Tabellen ins RTF-Format umsetzen.\nUnter den folgenden Links findet Ihr die Dokumentation der einzelnen Funktionen aufgelistet:\nAußerdem sind im Tutorial Beispiele für alle implementierten Verfahren und Tabellen zu finden.\nWir können zum Beispiel mit ezAnova eine Varianzanalyse für unsere Pinguine durchführen, bei der wir Pinguin-Geschlecht und Spezies als UVs und die Flossenlänge als AV nutzen:\nlibrary(ez)\n\npalmerpenguins::penguins %&gt;% \n  mutate(id = seq_along(species)) %&gt;% \n  filter(!is.na(flipper_length_mm)) %&gt;% \n  ezANOVA(wid = id,\n          between = .(species, sex),\n          dv = flipper_length_mm,\n          detailed = T,\n          type = 2)\n\nWarning: Converting \"id\" to factor for ANOVA.\n\n\nWarning: You have removed one or more levels from variable \"sex\". Refactoring\nfor ANOVA.\n\n\nWarning: Data is unbalanced (unequal N per group). Make sure you specified a\nwell-considered value for the type argument to ezANOVA().\n\n\nCoefficient covariances computed by hccm()\n\n\n$ANOVA\n       Effect DFn DFd        SSn      SSd          F             p p&lt;.05\n1     species   2 327 50185.0266 10458.11 784.582911 1.569568e-125     *\n2         sex   1 327  3905.6038 10458.11 122.118894  2.461150e-24     *\n3 species:sex   2 327   329.0425 10458.11   5.144186  6.314424e-03     *\n         ges\n1 0.82754673\n2 0.27190772\n3 0.03050319\n\n$`Levene's Test for Homogeneity of Variance`\n  DFn DFd      SSn    SSd        F          p p&lt;.05\n1   5 327 141.5881 3845.9 2.407723 0.03652301     *\nMit apa.ezANOVA.table können gewünschte Tabellen dann exportiert werden. Laut der Doku ist dabei noch wichtig, mit options eine Anzahl an Dezimalstellen vor Umwandlung in 10-er-Potenz-Notation zu setzen, die mindestens 10 ist.\nlibrary(apaTables)\noptions(digits = 10)\n\npalmerpenguins::penguins %&gt;%\n  mutate(id = seq_along(species)) %&gt;%\n  filter(!is.na(flipper_length_mm)) %&gt;%\n  ezANOVA(\n    wid = id,\n    between = .(species, sex),\n    dv = flipper_length_mm,\n    detailed = T,\n    type = 2\n  ) %&gt;%\n  apa.ezANOVA.table(drink_attitude_results,\n                    table.number = 1,\n                    filename = \"Table1_APA.doc\")\n\nWarning: Converting \"id\" to factor for ANOVA.\n\n\nWarning: You have removed one or more levels from variable \"sex\". Refactoring\nfor ANOVA.\n\n\nWarning: Data is unbalanced (unequal N per group). Make sure you specified a\nwell-considered value for the type argument to ezANOVA().\n\n\nCoefficient covariances computed by hccm()\n\n\n\n\nTable 1 \n\nANOVA results\n \n\n     Predictor df_num df_den   SS_num   SS_den      F    p ges\n       species      2    327 50185.03 10458.11 784.58 .000 .83\n           sex      1    327  3905.60 10458.11 122.12 .000 .27\n species x sex      2    327   329.04 10458.11   5.14 .006 .03\n\nNote. df_num indicates degrees of freedom numerator. df_den indicates degrees of freedom denominator. \nSS_num indicates sum of squares numerator. SS_den indicates sum of squares denominator. \nges indicates generalized eta-squared.\nDas table_number-Argument setzt dabei nur die Zahl in der Tabellen-Überschrift.\nDas Ergebnis ist in Abbildung 9.1 zu sehen.\nDas ist natürlich schon schön und praktisch wenn wir uns englische Tabellen für die implementierten Analysen ausgeben lassen wollen. Aber wie können wir eigene Tabellen nach APA-Richtlinien-konform exportieren?",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Erstellung von APA-konformen Tabellen</span>"
    ]
  },
  {
    "objectID": "tables.html#export-mit-apatables",
    "href": "tables.html#export-mit-apatables",
    "title": "Erstellung von APA-konformen Tabellen",
    "section": "",
    "text": "Korrelationsanalyse\nANOVA mit ezANOVA\nRegressionsanalyse\nZusammenfassungstabelle für deskriptive Maße, 1 Faktor\nZusammenfassungstabelle für deskriptive Maße, 2 Faktoren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbb 9.1: Output von apa.ezANOVA.table",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Erstellung von APA-konformen Tabellen</span>"
    ]
  },
  {
    "objectID": "tables.html#tabellen-mit-flextable",
    "href": "tables.html#tabellen-mit-flextable",
    "title": "Erstellung von APA-konformen Tabellen",
    "section": "Tabellen mit flextable",
    "text": "Tabellen mit flextable\nWas sind die Anforderungen an Tabellen laut APA? Konsultieren wir nochmal Julias1 Folien (Abbildung 9.2).\n\n\n\n\n\n\nAbb 9.2: Checklist für Tabellen aus Julias Folien\n\n\n\nUnsere Tabellen müssen also die folgenden Anforderungen erfüllen:\n\nJede Spalte muss eine Überschrift haben\ndie Überschriften müssen zentriert sein\n\nAußerdem kommen noch die folgenden Anforderungen an die Formatierung statistischer Ergebnisse2 hinzu:\n\nNamen statistischer Kennwerte sollen kursiv sein\nZahlen sollen auf den Wert gerundet werden, bei dem Präzision erhalten wird\nWerte die nicht größer als 1 werden können sollen keine Null vor dem Komma haben\n\nFangen wir mit der Formatierung der Nummern an. Als Beispiel haben wir die folgende Tabelle, in der die mittlere Schnabellänge und Standardabweichung pro Pinguin-Spezies und Beobachtungsort abgetragen sind:\n\nsummary_table &lt;- palmerpenguins::penguins %&gt;%\n  group_by(species, island) %&gt;%\n  summarise(across(matches(\"bill_length_mm\"),\n                   .fns = list(\n                     M = \\(x) mean(x, na.rm = T),\n                     SD = \\(x) sd(x, na.rm = T)\n                   ),\n                   .names = \"{.col}_{.fn}\")) %&gt;% # Damit Funktion hinten steht\n  pivot_wider(names_from = 'island',\n              values_from = 3:4,\n              names_glue = \"{island}_{.value}\") # Damit Insel vorne steht\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nsummary_table\n\n# A tibble: 3 × 7\n# Groups:   species [3]\n  species   Biscoe_bill_length_m…¹ Dream_bill_length_mm_M Torgersen_bill_lengt…²\n  &lt;fct&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Adelie                      39.0                   38.5                   39.0\n2 Chinstrap                   NA                     48.8                   NA  \n3 Gentoo                      47.5                   NA                     NA  \n# ℹ abbreviated names: ¹​Biscoe_bill_length_mm_M, ²​Torgersen_bill_length_mm_M\n# ℹ 3 more variables: Biscoe_bill_length_mm_SD &lt;dbl&gt;,\n#   Dream_bill_length_mm_SD &lt;dbl&gt;, Torgersen_bill_length_mm_SD &lt;dbl&gt;\n\n\nZuerstmal sortieren wir die Spalten so, dass Pro Ort erst Mittelwert, dann SD steht:\n\nsummary_table %&gt;% \n  select(species, matches('Bis'), matches('Dre'), matches('Tor'))\n\n# A tibble: 3 × 7\n# Groups:   species [3]\n  species   Biscoe_bill_length_m…¹ Biscoe_bill_length_m…² Dream_bill_length_mm_M\n  &lt;fct&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Adelie                      39.0                   2.48                   38.5\n2 Chinstrap                   NA                    NA                      48.8\n3 Gentoo                      47.5                   3.08                   NA  \n# ℹ abbreviated names: ¹​Biscoe_bill_length_mm_M, ²​Biscoe_bill_length_mm_SD\n# ℹ 3 more variables: Dream_bill_length_mm_SD &lt;dbl&gt;,\n#   Torgersen_bill_length_mm_M &lt;dbl&gt;, Torgersen_bill_length_mm_SD &lt;dbl&gt;\n\n\nDann legen wir die Dezimalstellen auf eine Nachkommastelle fest:\n\nsummary_table %&gt;% \n  select(species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1)))\n\n# A tibble: 3 × 7\n# Groups:   species [3]\n  species   Biscoe_bill_length_m…¹ Biscoe_bill_length_m…² Dream_bill_length_mm_M\n  &lt;fct&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 Adelie                      39                      2.5                   38.5\n2 Chinstrap                   NA                     NA                     48.8\n3 Gentoo                      47.5                    3.1                   NA  \n# ℹ abbreviated names: ¹​Biscoe_bill_length_mm_M, ²​Biscoe_bill_length_mm_SD\n# ℹ 3 more variables: Dream_bill_length_mm_SD &lt;dbl&gt;,\n#   Torgersen_bill_length_mm_M &lt;dbl&gt;, Torgersen_bill_length_mm_SD &lt;dbl&gt;\n\n\nUnd schon können wir an die eigentliche Formatierung in einer Tabelle gehen. Dazu nutzen wir das Paket flextable.\nWir können unsere Tabelle direkt in flextable pipen:\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\nsummary_table %&gt;% \n  select(species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  flextable()\n\nspeciesBiscoe_bill_length_mm_MBiscoe_bill_length_mm_SDDream_bill_length_mm_MDream_bill_length_mm_SDTorgersen_bill_length_mm_MTorgersen_bill_length_mm_SDAdelie39.02.538.52.5393Chinstrap48.83.3Gentoo47.53.1\n\n\nAls erstes können wir den Header trennen:\n\nsummary_table %&gt;% \n  select(species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  flextable() %&gt;% \n  separate_header()\n\nspeciesBiscoeDreamTorgersenbilllengthmmMSDMSDMSDAdelie39.02.538.52.5393Chinstrap48.83.3Gentoo47.53.1\n\n\nDie bill-length kann weg, am besten entfernen wir die schon vor der Umwandlung in eine flextable:\n\nsummary_table %&gt;% \n  select(species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  rename_with(.fn = ~str_remove(.,'bill_length_mm')) %&gt;% \n  flextable() %&gt;% \n  separate_header()\n\nspeciesBiscoeDreamTorgersenMSDMSDMSDAdelie39.02.538.52.5393Chinstrap48.83.3Gentoo47.53.1\n\n\n“Spezies” können wir auch in deutsch übertiteln:\n\nsummary_table %&gt;% \n  select(Spezies = species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  rename_with(.fn = ~str_remove(.,'_bill_length_mm')) %&gt;% \n  flextable() %&gt;% \n  separate_header()\n\nSpeziesBiscoeDreamTorgersenMSDMSDMSDAdelie39.02.538.52.5393Chinstrap48.83.3Gentoo47.53.1\n\n\nUnd die statistischen Kennwerte kursiv setzen:\n\nsummary_table %&gt;% \n  select(Spezies = species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  rename_with(.fn = ~str_remove(.,'_bill_length_mm')) %&gt;% \n  flextable() %&gt;% \n  separate_header() %&gt;% \n  style(i = 2,part = 'header',\n        pr_t = fp_text_default(italic = T))\n\nSpeziesBiscoeDreamTorgersenMSDMSDMSDAdelie39.02.538.52.5393Chinstrap48.83.3Gentoo47.53.1\n\n\nIm flextable-Paket gibt es außerdem die theme_apa-Funktion, die den Text und die Abstände nach APA formatiert:\n\nsummary_table %&gt;% \n  select(Spezies = species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  rename_with(.fn = ~str_remove(.,'_bill_length_mm')) %&gt;% \n  flextable() %&gt;% \n  separate_header() %&gt;%\n  italic(part =\"header\", i= 2) %&gt;% \n  theme_apa()\n\nSpeziesBiscoeDreamTorgersenMSDMSDMSDAdelie39.002.5038.502.5039.003.00Chinstrap48.803.30Gentoo47.503.10\n\n\nAußerdem können wir Linien unter den Inseln einfügen:\n\nsummary_table %&gt;% \n  select(Spezies = species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  rename_with(.fn = ~str_remove(.,'_bill_length_mm')) %&gt;% \n  flextable() %&gt;% \n  separate_header() %&gt;%\n  italic(part =\"header\", i= 2) %&gt;% \n  theme_apa() %&gt;% \n  hline(i = 1, j = -1,part = 'header', \n        border= list(width = 0.1, color = \"black\", style = \"solid\"))\n\nSpeziesBiscoeDreamTorgersenMSDMSDMSDAdelie39.002.5038.502.5039.003.00Chinstrap48.803.30Gentoo47.503.10\n\n\nUnd abschließend exportieren:\n\nsummary_table %&gt;% \n  select(Spezies = species, matches('Bis'), matches('Dre'), matches('Tor')) %&gt;% \n  mutate(across(where(is.numeric), ~round(., 1))) %&gt;% \n  rename_with(.fn = ~str_remove(.,'_bill_length_mm')) %&gt;% \n  flextable() %&gt;% \n  separate_header() %&gt;%\n  italic(part =\"header\", i= 2) %&gt;% \n  theme_apa() %&gt;% \n  hline(i = 1, j = -1,part = 'header', \n        border= list(width = 0.1, color = \"black\", style = \"solid\")) %&gt;% \n  save_as_docx(path = 'flextable_out.docx')\n\nDer Export ist in Abbildung 9.3 zu sehen.\n\n\n\n\n\n\nAbb 9.3: Export des flextable-calls.\n\n\n\n\n\n\n\n\nAber da muss ich ja die Beschreibung doch noch nachträglich einfügen! Und was soll ich mit 15 einzelnen docs, dann muss ich ja doch alles rüberkopieren!\n\n\n\nNatürlich gibt es da auch eine Lösung! Auftritt quarto.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Erstellung von APA-konformen Tabellen</span>"
    ]
  },
  {
    "objectID": "tables.html#aufgabe",
    "href": "tables.html#aufgabe",
    "title": "Erstellung von APA-konformen Tabellen",
    "section": "Aufgabe",
    "text": "Aufgabe\n\nErstelle eine Tabelle mit den deskriptiven Kennwerten der Blütenblatt-Länge im iris-Datensatz mit apaTables. Guck Dir dafür die Dokumentation der apa.1way.table-Funktion an.\nBaue dieselbe Tabelle mit flextable nach.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Erstellung von APA-konformen Tabellen</span>"
    ]
  },
  {
    "objectID": "tables.html#footnotes",
    "href": "tables.html#footnotes",
    "title": "Erstellung von APA-konformen Tabellen",
    "section": "",
    "text": "Nochmal Danke, Julia!↩︎\nlaut https://apastyle.apa.org/style-grammar-guidelines/tables-figures/tables und https://www.scribbr.com/apa-style/numbers-and-statistics/↩︎",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Erstellung von APA-konformen Tabellen</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Methodenteil in quarto",
    "section": "",
    "text": "Code-chunks\nDer große Vorteil quartos ist es, dass wir Code-chunks im Dokument anlegen können, deren Output direkt in das gerenderte Ergebnis eingebunden wird.\nDazu kann mit der “Insert new code chunk”-Schaltfläche (1 in Abbildung 10.3) oben rechts oder Strg + Alt + I ein neuer Chunk eingefügt werden, der an die Cursor-Stelle im Dokument eingefügt wird (2).\nIn der ersten Zeile des neuen Chunks steht die Sprache. In den Chunk kann dann wie in ein R-Skript Code eingefügt werden.\nDamit haben wir die Basics die wir brauchen.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#code-chunks",
    "href": "quarto.html#code-chunks",
    "title": "Methodenteil in quarto",
    "section": "",
    "text": "Abb 10.3: Neuen Chunk erstellen.\n\n\n\n\n\n\n\n\n\n\n\n\nAbb 10.4: Code Chunks mit bisherigen Analysen.\n\n\n\n\nIn Abbildung 10.4 ist das neue file mit dem Code für die letzte Tabelle aus Erstellung von APA-konformen Tabellen und der Beispielgrafik aus Die Grammar of graphics und ggplot2 eingefügt.\nDer erste Chunk (1) ist dabei nur dafür da um Pakete und die Daten zu laden. Im zweiten Chunk (2) wird die Tabelle, im dritten Chunk (3) die Grafik erstellt. Der Text hinter dem r in der ersten Zeile der Chunks ist nur ein Titel und hat erstmal keine weitere Bedeutung.\nWie in (4) und (5) zu sehen ist, kann zwischen den Chunks einfach Text eingefügt werden.\nDie Chunks können dabei wie ein ganz normales R-Skript genutzt werden und der Code an der Zeile des Cursors kann mit Strg + Enter ausgeführt werden. Der Output erscheint dabei unter dem Chunk, kann mit Hilfe der Einstellungen aber auch in die Konsole verschoben werden.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#yaml-header",
    "href": "quarto.html#yaml-header",
    "title": "Methodenteil in quarto",
    "section": "YAML-Header",
    "text": "YAML-Header\nUm das Ergebnis des Renderns anzupassen können wir den YAML-Header setzen.\nFür dieses Skript konzentrieren wir uns auf Output im docx-Format, Quarto bietet aber wesentlich mehr Optionen. Die Doku für alle unterstützten Header-Optionen für docx findet sich hier.\nFür uns sind erstmal die folgenden Parameter wichtig:\nformat: docx # setzt das Format des Outputs\nfig-width: 6.56 # setzt 6.56 Zoll als Standard-Breite für alle Plot-Outputs\nfig-dpi: 300 # setzt alle plots auf 300 dpi\nNach Klick auf “Render” kann ich neben dem erstellten qmd-file das in Abbildung 10.5 abgebildete Dokument finden.\n\n\n\n\n\n\nAbb 10.5: Erstes Render-Ergebnis.\n\n\n\nDer Output der Chunks wurde also erfolgreich in das Dokument übergeben, ich brauche aber die ganzen messages nicht.\nUm diese zu unterdrücken kann ich den YAML-Header einfach um den folgenden Teil ergänzen:\nexecute:\n  warning: false\n  message: false\nWas zu folgendem Output führt:\n Schon viel besser, für einen Methodenteil stört aber noch der Code zwischen den Outputs. Um das rausschreiben der Chunks zu unterdrücken ergänzen wir nur noch echo: false um bei folgendem vollständigen Header zu enden:\ntitle: \"Methoden\"\neditor: visual\nformat: docx\nfig-width: 6.56\nfig-dpi: 300\nexecute:\n  warning: false\n  message: false\n  echo: false\nUnd schon ist das Ergebnis nicht mehr allzu schlecht:\n\n\n\n\n\n\nAbb 10.6: Ergebnis ohne Chunks",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#code-chunk-optionen",
    "href": "quarto.html#code-chunk-optionen",
    "title": "Methodenteil in quarto",
    "section": "Code-Chunk-Optionen",
    "text": "Code-Chunk-Optionen\nDie letzten drei Parameter waren Beispiele für chunk-Optionen, die das Verhalten von Chunks anpassen. Das kann entweder global im YAML-Header unter execute oder lokal pro Chunk gesetzt werden.\nAuf der quarto-Seite gibt es einen Überblick über alle Optionen, wir konzentrieren uns aber erstmal auf einen kleinen Teil zur Beschriftung von Grafiken und Tabellen.\nUm eine Grafik zu beschriften und im Text referenzierbar zu machen,müssen wir zwei Optionen für den Chunk setzten: label und fig-cap. In Abbildung 10.7 ist ein Beispiel wie das aussehen könnte. Die Chunk-Optionen werden dabei immer durch ein so genanntes “pipe-comment” eingeleitet.\n\n\n\n\n\n\nAbb 10.7: Chunk-Optionen für Grafiken\n\n\n\nIm text können wir die Grafik dann mit @fig-scatter referenzieren. Das label für Grafiken muss dabei durch “fig-” eingeleitet und referenziert werden, sonst schlägt die Formatierung fehl.\nÄhnliche Parameter sind auch für Tabellen implementiert, hier ist der Prefix nur tbl. Leider funktioniert das Crossreferenzieren in den aktuellen Versionen von quarto und flextable noch nicht reibungslos, es gibt aber einen workaround.\nStatt mit @tbl-table müssen wir die Tabelle mit einem Call an das officer-Paket im Text referenzieren. Sonst ist das Format aber ähnlich, wie in Abbildung 10.8 zu sehen ist..\n\n\n\n\n\n\nAbb 10.8: Chunk-Optionen für Tabellen\n\n\n\nDer Output des so angepassten Skripts ist in Abbildung 10.9 zu sehen.\n\n\n\n\n\n\nAbb 10.9: Ergebnis mit Corss-Referenzen\n\n\n\nOffensichtlich funktioniert auch der Workaround noch nicht perfekt - das Problem ist aber bekannt.\nIn dem Beispiel haben wir aber einen weiteren Vorteil von Quarto gesehen: Inline Code-Chunks",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-inline",
    "href": "quarto.html#sec-inline",
    "title": "Methodenteil in quarto",
    "section": "Inline Code-Chunks",
    "text": "Inline Code-Chunks\nIm Fließtext kann mit der folgenden Syntax auf R zugegriffen und Output generiert werden:\n`r &lt;some code&gt;`\nWozu ist das nützlich? Wir können so einfach statistische Kennwerte ohne sie kopieren zu müssen in den Text einflißen lassen! Und jedes mal wenn das Ergebnis erstellt wird, werden die Werte neu berechnet. So kann keine Aktualisierung vergessen werden.\nWir könnten unserem Beispiel-File so beispielsweise eine Stichprobenbeschreibung hinzufügen, wie in Abbildung 10.10 zu sehen ist.\n\n\n\n\n\n\nAbb 10.10: Inline Chunk\n\n\n\nDie Ergebnisse im docx können in Abbildung 10.11 gesehen werden.\n\n\n\n\n\n\nAbb 10.11: Gerendertes Ergebnis der Inline-Chunks\n\n\n\nDas hier gezeigte Beispiel ist auch im Repo des Skripts zugänglich.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#aufgabe",
    "href": "quarto.html#aufgabe",
    "title": "Methodenteil in quarto",
    "section": "Aufgabe",
    "text": "Aufgabe\n\nÜbertrage Deine Ergebnisse aus den anderen Aufgaben in ein quarto-Dokument\nBeschrifte alle Grafiken und Tabellen.\nRender das Dokument als docx.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#zusammenarbeit",
    "href": "quarto.html#zusammenarbeit",
    "title": "Methodenteil in quarto",
    "section": "Zusammenarbeit",
    "text": "Zusammenarbeit\nDer größte Nachteil an Quarto ist der etwas umständlichere Workflow beim Kommentieren und Zusammenarbeiten als das in Word möglich ist.\nDie Dokumente können zwar immer in word gerendert und dann ausgetauscht werden, etwaige Anmerkungen können aber nicht direkt in Quarto eingepflegt werden.\nUnter diesem Link findet Ihr aber eine Anleitung, wie man die Zusammenarbeit über git ermöglichen kann.",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "Methodenteil in quarto",
    "section": "",
    "text": "What you see is what you get↩︎",
    "crumbs": [
      "Thesis mit R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Methodenteil in `quarto`{#sec-quarto}</span>"
    ]
  },
  {
    "objectID": "intro_lit.html",
    "href": "intro_lit.html",
    "title": "Literaturrecherche",
    "section": "",
    "text": "Die folgenden Kapitel beschäftigen sich mit dem Finden, dem Sammeln und dem Lesen von psychologischen Forschungsarbeiten.\nDazu haben die Kursteilnehmenden und Mitarbeitenden im Vorfeld eine Umfrage zu Ihren jeweiligen Strategien ausgefüllt. Hier werdet Ihr also viele Sachen finden, die Euch bekannt vorkommen - hoffentlich sind aber auch Tipps und Vorschläge dabei, die Euch weiterhelfen.",
    "crumbs": [
      "Literaturrecherche"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vorschläge zur Thesis-Bearbeitung",
    "section": "",
    "text": "Vorwort\nDieses mit quarto erstellte Dokument wurde als Sammlung von Vorschlägen und Tipps zur Erstellung von Abschlussarbeiten im Rahmen des Betreuungskolloquiums der Abteilung für Psychologische Methodenlehre der CAU zu Kiel erstellt.",
    "crumbs": [
      "Vorwort"
    ]
  },
  {
    "objectID": "index.html#lernziele",
    "href": "index.html#lernziele",
    "title": "Vorschläge zur Thesis-Bearbeitung",
    "section": "Lernziele",
    "text": "Lernziele\n\nLiteraturrecherche\n\nLiteratur finden\nIn diesem Abschnitt wird ein kurzer Abriss über Strategien zur Suche von Literatur gegeben.\nNach dem Lesen des Abschnitts sollten Leser:innen dazu in der Lage sein, Suchmaschinen und Literatur-Datenbanken zu ihrem Vorteil zu nutzen, um Literatur zu einem Thema zu suchen und weitergehende Literatur zu sammeln.\n\n\nLiteratur sammeln\nDieser Abschnitt gibt einen Überblick über die möglichen Quellen für Literatur und eine Einführung in Literaturverwaltungsprogramme anhand von Zotero als Beispiel.\nMit diesem Abschnitt wird versucht, den Leser:innen eine Auswahl an Literaturquellen zugänglich zu machen. Außerdem sollen Leser:innen die Vorteile von Literaturverwaltungsprogrammen verstehen und wissen, warum sie diese einsetzen sollten.\n\n\nLiteratur lesen\nAls Abschluss zur Literaturrecherche wird in diesem Abschnitt auf Strategien zum effizienten Lesen psychologischer Forschungsartikel eingegangen. Außerdem geht das Kapitel auf Strategien zur Erstellung von Notizen ein.\nAm Ende dieses Abschnittes sollten Leser:innen in der Lage sein, effizient psychologische Forschungsarbeiten zum Verschaffen eines Überblicks zu lesen.\n\n\n\nThesis mit R\n\nImport und Aufbereitung von Psychopy-Daten\nDieser Abschnitt beschäftigt sich ausschließlich mit dem Stapel-Import eines Haufens an Dateien einzelner Proband:innen. Die Beispiele beziehen sich auch Psychopy/Pavlovia-typische Datensammlungen, können aber auch auf andere Beispiele angewandt werden.\nDas Lesen des Abschnittes sollte Leser:innen dazu befähigen, purrr- und readr-Funktionalitäten zu nutzen, um alle in einem Ordner vorliegenden Dateien zu importieren.\n\n\nGrafiken mit ggplot2\nIn diesem Abschnitt wird ein kurzer Abriss zur Auffrischung der Logik hinter der grammar of graphics präsentiert. Anschließend werden Beispiele für die Erstellung einiger der häufigsten Grafik-Typen gezeigt. Abschließend wird ein Überblick für potentiell interessante andere Stellen zur Referenz eingeführt.\nNach diesem Abschnitt sollten die grundlegenden Prinzipien zur Erstellung von Grafiken mit ggplot2 kennen und auf häufig genutzte Grafikformate anwenden können. Außerdem sollte ein Überblick an möglichen anderen Stellen zur weiteren Information gewonnen sein.\n\n\nErstellung von APA-konformen Tabellen\nDieser Abschnitt beschäftigt sich mit der Erstellung APA-konformer Tabellen in R. Dazu wird zum Einen auf den spezifischen Export mit apaTables, zum Anderen auf den flexibleren Ansatz mit flextable eingegangen.\nNach diesem Abschnitt sollten in der Lage sein, Tabellen gängiger Analysen aus R im APA-Format zu exportieren. Außerdem soll die Fähigkeit vermittelt werden, die Erstellung von APA-konformen Tabellen mit flextable durchzuführen.\n\n\nMethodenteil in quarto\nDieser Abschnitt beschäftigt sich mit der Erstellung von Textabschnitten oder ganzer Arbeiten mit quarto, dem Texterstellungs-Framework, das von Posit zur direkten Erstellung von Texten in RStudio unterstützt wird.\nAm Ende dieses Abschnittes sollten in der Lage sein quarto-Dokumente zu erstellen, ihre Auswertung in Code-chunks durchführen können und die Ergebnisse in pdf und word exportieren können.",
    "crumbs": [
      "Vorwort"
    ]
  }
]